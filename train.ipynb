{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1(tp, fp, fn):\n",
    "    if (tp + 0.5 * (fp + fn)) == 0:\n",
    "        return 0\n",
    "    return tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "def augment_text(row, deletion_prob=0.05, swap_prob=0.3):\n",
    "    # Tokenize the text\n",
    "    tokens = row['text'].split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        if random.random() < swap_prob:\n",
    "            swap_i = random.randint(0, len(tokens) - 1)\n",
    "            if swap_i != i:  # Ensure not swapping with itself\n",
    "                tokens[i], tokens[swap_i] = tokens[swap_i], tokens[i]\n",
    "    \n",
    "    # Apply deletion\n",
    "    tokens = [token for token in tokens if random.random() >= deletion_prob]\n",
    "    \n",
    "    # Reconstruct the augmented text\n",
    "    augmented_text = ' '.join(tokens)\n",
    "    return augmented_text\n",
    "\n",
    "def train_deberta(learning_rate, batch_size, num_epochs, weight_decay, dropout, num_layers_unfrozen,\n",
    "                  train_file='data/train_paraphrase_upsampled.csv', test_file='data/dev_set.csv',\n",
    "                  save_path=None, test_results_path='dev.txt'):\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    # Model configuration with hyperparameters\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"output_dir\": f\"outputs_lr_{learning_rate}_batch_size_{batch_size}_unfreeze_{num_layers_unfrozen}\",\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_best_model\": True,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_model_every_epoch\": False,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "        \"use_multiprocessing_for_evaluation\": False,\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"fp16\": False,  # Ensure FP16 is disabled\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "\n",
    "    # Initialize DeBERTa model\n",
    "    model = ClassificationModel(\n",
    "        \"deberta\",  # Model type\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=2,\n",
    "        args=model_args,\n",
    "        # use_cuda=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    # Unfreeze the last `num_layers_unfrozen` layers + classifier head\n",
    "    model_layers = list(model.model.deberta.encoder.layer)\n",
    "    num_total_layers = len(model_layers)\n",
    "    layers_to_unfreeze = min(num_layers_unfrozen, num_total_layers)\n",
    "\n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = False  # Freeze everything first\n",
    "\n",
    "    for i in range(num_total_layers - layers_to_unfreeze, num_total_layers):\n",
    "        for param in model_layers[i].parameters():\n",
    "            param.requires_grad = True  # Unfreeze selected layers\n",
    "\n",
    "    # Ensure classifier head is always trainable\n",
    "    for name, param in model.model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    cols = [\"text\", \"label\"]\n",
    "    \n",
    "    # Train for the required number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        _train_df = train_df.copy()\n",
    "\n",
    "        if epoch > 4:\n",
    "            _train_df[\"text\"] = _train_df.apply(lambda row: augment_text(row), axis=1)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train_model(_train_df[cols])\n",
    "        \n",
    "    print(\"Evaluating on dev set\")\n",
    "    preds, _ = model.predict(test_df[cols])\n",
    "\n",
    "    with open(test_results_path, 'w+') as f:\n",
    "        for pred in preds:\n",
    "            f.write(pred + '\\n')\n",
    "    \n",
    "    if save_path:\n",
    "        print(\"Saving final model to\", save_path)\n",
    "        model.model.save_pretrained(save_path)\n",
    "        model.tokenizer.save_pretrained(save_path)\n",
    "        model.config.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
