{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2094 entries, 0 to 2093\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   par_id      2094 non-null   int64 \n",
      " 1   community   2094 non-null   object\n",
      " 2   text        2094 non-null   object\n",
      " 3   label       2094 non-null   int64 \n",
      " 4   orig_label  2094 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 81.9+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836223/2109743803.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['text'].fillna('', inplace=True)\n",
      "/tmp/ipykernel_836223/2109743803.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  val_df['text'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "partial_train_df = pd.read_csv('train_set.csv')\n",
    "internal_dev_df = pd.read_csv('internal_validation_set.csv')\n",
    "val_df = pd.read_csv('dev_set.csv')\n",
    "\n",
    "# Combine partial train with internal dev to make the train set\n",
    "train_df = pd.concat([partial_train_df, internal_dev_df], ignore_index=True)\n",
    "\n",
    "# Replace NaNs with empty strings\n",
    "train_df['text'].fillna('', inplace=True)\n",
    "val_df['text'].fillna('', inplace=True)\n",
    "\n",
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(df, k=5, output_prefix=\"fold\"):\n",
    "    \"\"\"\n",
    "    Splits dataset into stratified K folds, relying on StratifiedKFold to maintain class balance.\n",
    "    \"\"\"\n",
    "    assert \"text\" in df.columns and \"label\" in df.columns, \"Dataset must contain 'text' and 'label' columns.\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_idx = 1\n",
    "\n",
    "    for train_index, val_index in skf.split(df[\"text\"], df[\"label\"]):\n",
    "        train_df, val_df = df.iloc[train_index], df.iloc[val_index]\n",
    "\n",
    "        train_df.to_csv(f\"{output_prefix}_train_{fold_idx}.csv\", index=False)\n",
    "        val_df.to_csv(f\"{output_prefix}_val_{fold_idx}.csv\", index=False)\n",
    "        print(f\"Saved Fold {fold_idx}: {len(train_df)} training, {len(val_df)} validation\")\n",
    "        fold_idx += 1\n",
    "\n",
    "# create_folds(train_df, k=5, output_prefix=\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the positive class F1-score\n",
    "def f1(preds, labels):\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    return classification_report(labels, preds, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "\n",
    "def train_deberta(fold_idx, learning_rate, batch_size, num_epochs, weight_decay, num_layers_unfrozen, log_prefix=\"log_lr_\"):\n",
    "    # Load dataset\n",
    "    train_file = f\"fold_train_{fold_idx}.csv\"\n",
    "    val_file = f\"fold_val_{fold_idx}.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    val_df = pd.read_csv(val_file)\n",
    "\n",
    "    # Upsample the minority class within the training set\n",
    "    df_majority = train_df[train_df[\"label\"] == 0]\n",
    "    df_minority = train_df[train_df[\"label\"] == 1]\n",
    "    target_minority_size = int(len(df_majority) * (2 / 3)) # Ratio 2:3\n",
    "\n",
    "    duplication_factor = target_minority_size // len(df_minority)\n",
    "    remainder = target_minority_size % len(df_minority)\n",
    "    df_minority_upsampled = pd.concat([df_minority] * duplication_factor, ignore_index=True)\n",
    "    df_minority_upsampled = pd.concat([df_minority_upsampled, df_minority.sample(n=remainder, random_state=42)], ignore_index=True)\n",
    "\n",
    "    train_df_upsampled = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Fold {fold_idx}: Training set size after upsampling: {len(train_df_upsampled)} ({train_df_upsampled['label'].value_counts()}), Validation set size: {len(val_df)} ({val_df['label'].value_counts()})\")\n",
    "\n",
    "    # Model configuration with hyperparameters\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": num_epochs,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"output_dir\": f\"outputs_lr_{learning_rate}_batch_size_{batch_size}_unfreeze_{num_layers_unfrozen}/fold_{fold_idx}\",\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_best_model\": False,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_model_every_epoch\": False,\n",
    "        \"early_stopping_patience\": 3,  # Stop if no improvement after 3 eval steps\n",
    "        \"evaluate_during_training\": False,\n",
    "        # \"evaluate_during_training_steps\": 2000,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "        \"use_multiprocessing_for_evaluation\": False,\n",
    "        # \"early_stopping_metric\": \"f1\",\n",
    "        # \"early_stopping_metric_minimize\": False, # Maximise the F1-score\n",
    "        \"classification_report\": True,\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"fp16\": False  # Ensure FP16 is disabled\n",
    "    }\n",
    "\n",
    "    # Initialize DeBERTa model\n",
    "    model = ClassificationModel(\n",
    "        \"deberta\",  # Model type\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=2,\n",
    "        args=model_args,\n",
    "        use_cuda=True\n",
    "    )\n",
    "\n",
    "    # Unfreeze the last `num_layers_unfrozen` layers + classifier head\n",
    "    model_layers = list(model.model.deberta.encoder.layer)\n",
    "    num_total_layers = len(model_layers)\n",
    "    layers_to_unfreeze = min(num_layers_unfrozen, num_total_layers)\n",
    "\n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = False  # Freeze everything first\n",
    "\n",
    "    for i in range(num_total_layers - layers_to_unfreeze, num_total_layers):\n",
    "        for param in model_layers[i].parameters():\n",
    "            param.requires_grad = True  # Unfreeze selected layers\n",
    "\n",
    "    # Ensure classifier head is always trainable\n",
    "    for name, param in model.model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(\n",
    "        train_df_upsampled[[\"text\", \"label\"]],\n",
    "        # eval_df=val_df[[\"text\", \"label\"]],\n",
    "        f1=f1\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    # result, model_outputs, wrong_predictions = model.eval_model(val_df)\n",
    "    # print(\"Validation Results:\", result)\n",
    "\n",
    "    texts = val_df['text'].tolist()\n",
    "\n",
    "    # Get predictions\n",
    "    preds, raw = model.predict(texts)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(val_df['label'], preds, digits=4, output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(val_df['label'], preds, digits=4))\n",
    "\n",
    "    # Log results to a separate file per job\n",
    "    base_prefix = \"./\"\n",
    "    log_file = f\"{base_prefix}{log_prefix}{learning_rate}.csv\"\n",
    "    log_data = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_layers_unfrozen\": num_layers_unfrozen,\n",
    "        \"f1_class_0\": report[\"0\"][\"f1-score\"],\n",
    "        \"f1_class_1\": report[\"1\"][\"f1-score\"],\n",
    "        \"precision_class_0\": report[\"0\"][\"precision\"],\n",
    "        \"precision_class_1\": report[\"1\"][\"precision\"],\n",
    "        \"recall_class_0\": report[\"0\"][\"recall\"],\n",
    "        \"recall_class_1\": report[\"1\"][\"recall\"],\n",
    "        \"overall_f1\": report[\"macro avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "    log_df = pd.DataFrame([log_data])\n",
    "    log_df.to_csv(log_file, index=False)\n",
    "\n",
    "    return report[\"1\"][\"f1-score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Training set size after upsampling: 10106 (label\n",
      "0    6064\n",
      "1    4042\n",
      "Name: count, dtype: int64), Validation set size: 1675 (label\n",
      "0    1517\n",
      "1     158\n",
      "Name: count, dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ian/uni/NLP-Binary-Classifier-CW/venv/lib/python3.12/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7294441241bc44cfa446047cccb944fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647b13124ff94485a4045de4a596907f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m f1_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):  \u001b[38;5;66;03m# Assuming 5 folds\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_deberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers_unfrozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers_unfrozen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         f1_scores\u001b[38;5;241m.\u001b[39mappend(f1_score)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print the average F1-score for this combination\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 82\u001b[0m, in \u001b[0;36mtrain_deberta\u001b[0;34m(fold_idx, learning_rate, batch_size, num_epochs, weight_decay, num_layers_unfrozen, log_prefix)\u001b[0m\n\u001b[1;32m     79\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df_upsampled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_df=val_df[[\"text\", \"label\"]],\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# result, model_outputs, wrong_predictions = model.eval_model(val_df)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# print(\"Validation Results:\", result)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m texts \u001b[38;5;241m=\u001b[39m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/uni/NLP-Binary-Classifier-CW/venv/lib/python3.12/site-packages/simpletransformers/classification/classification_model.py:630\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    622\u001b[0m     train_dataset,\n\u001b[1;32m    623\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    624\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    625\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    626\u001b[0m )\n\u001b[1;32m    628\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# self.tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/uni/NLP-Binary-Classifier-CW/venv/lib/python3.12/site-packages/simpletransformers/classification/classification_model.py:942\u001b[0m, in \u001b[0;36mClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 942\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfp16:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00002\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "weight_decay = 0.0\n",
    "num_layers_unfrozen = 5\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for i in range(1, 6):  # Assuming 5 folds\n",
    "        f1_score = train_deberta(fold_idx=i, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs, weight_decay=weight_decay, num_layers_unfrozen=num_layers_unfrozen)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "# Print the average F1-score for this combination\n",
    "print(f\"Average F1-score for lr={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}, weight_decay={weight_decay}, num_layers_unfrozen={num_layers_unfrozen}: {np.mean(f1_scores)}\")\n",
    "print(f1_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
