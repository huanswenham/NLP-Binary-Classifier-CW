{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_set.csv\")\n",
    "test_df = pd.read_csv(\"data/dev_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "train_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\")\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation and special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df[\"processed_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"processed_text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train = train_df[\"processed_text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = test_df[\"processed_text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag-of-Words features\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Vocabulary Size:\", len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression Model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "class DeepSeekApi:\n",
    "  def __init__(self):\n",
    "    # Obtain API key\n",
    "    load_dotenv()\n",
    "    self.api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "  # Function to call DeepSeek API to rephrase text\n",
    "  def rephrase(self, text):\n",
    "    prompt = f\"rephrase: {text}\"\n",
    "    return self._call_api(prompt)\n",
    "  \n",
    "  # Function to call DeepSeek API to classify if text is PCL or not (baseline)\n",
    "  def pcl_classify(self, text):\n",
    "    prompt = f\"Accoording to the paper \\\"Don’t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities\\\", please classify this sentence \\\"{text}\\\" on wheter it is considered a Patronizing and Condescendig Language (PCL) or not. Just reply me with either \\\"True\\\" if you think this is ccnsiderd PCL, or \\\"False\\\" if you think otherwise.\"\n",
    "    return self._call_api(prompt)\n",
    "\n",
    "  # Function to call DeepSeek API to classify if text is PCL or not in batches (baseline)\n",
    "  def batch_pcl_classify(self, texts, batch_size):\n",
    "    prompts = []\n",
    "    for text in texts:\n",
    "      prompt = f\"Accoording to the paper \\\"Don’t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities\\\", please classify this sentence \\\"{text}\\\" on wheter it is considered a Patronizing and Condescendig Language (PCL) or not. Just reply me with either \\\"True\\\" if you think this is ccnsiderd PCL, or \\\"False\\\" if you think otherwise.\"\n",
    "      prompts.append(prompt)\n",
    "    return self._batch_call_api(prompts, batch_size)\n",
    "  \n",
    "  def _call_api(self, prompt):\n",
    "    deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "    client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "  def _batch_call_api(self, prompts, batch_size):\n",
    "    deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "    client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "    \n",
    "    messages = []\n",
    "    for prompt in prompts:\n",
    "      messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Original: {response.choices[0].message.content}\")\n",
    "    predictions = response.choices[0].message.content.split(' \\n')\n",
    "    logging.info(f\"After split: {predictions}\")\n",
    "    predictions = list(map(lambda s: \"\".join(s.split()), predictions))\n",
    "    logging.info(f\"After remove space: {predictions}\")\n",
    "    if '.' in predictions[0]:\n",
    "      predictions = list(map(lambda s: \"\".join(s.split('.')[1:]), predictions))\n",
    "      logging.info(f\"After remove dot: {predictions}\")\n",
    "    # Filter out the empty strings\n",
    "    predictions = [s for s in predictions if s != '']\n",
    "    logging.info(f\"After remove empty str: {predictions}\")\n",
    "    \n",
    "    logging.info(predictions)\n",
    "    \n",
    "    if len(predictions) != batch_size:\n",
    "      raise ValueError(\"Error processing predictions from DeepSeek response.\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_PARAPHRASES = 6\n",
    "BATCH_SIZE = 1\n",
    "LOG_FILEPATH = 'deepseek_api_logs/deepseek_dev_baseline.log'\n",
    "SAVE_CSV_FILEPATH = 'data/deepseek_dev_baseline.csv'\n",
    "\n",
    "def add_predictions_to_new_df(predictions, index, old_df, new_df):\n",
    "    for i in range(index,  min(index + BATCH_SIZE, len(old_df))):\n",
    "        entry = old_df.iloc[[i], [4]].copy()\n",
    "        entry.at[entry.index[0], 'prediction'] = predictions[i - index]\n",
    "        new_df = pd.concat([new_df, entry], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def calc_f1(preds, labels):\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    return classification_report(labels, preds, output_dict=True)[\"1\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the training dataset\n",
    "dev_df = pd.read_csv(\"../data/dev_set.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.DataFrame(dev_df).reset_index(drop=True)\n",
    "\n",
    "# Initialise a new empty dataframe\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# Initialise DeepSeek api\n",
    "deepseek = DeepSeekApi()\n",
    "\n",
    "# Set up logging to a file\n",
    "logging.basicConfig(filename=LOG_FILEPATH, level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "index = 0\n",
    "while index < len(dev_df):\n",
    "    try:\n",
    "        logging.info(f\"Processing from index {index}.\")\n",
    "        texts = []\n",
    "        for i in range(index, min(index + BATCH_SIZE, len(dev_df))):\n",
    "            text = dev_df.iloc[i, 4]\n",
    "            texts.append(text)\n",
    "            logging.info(f\"Sentence: {text}\")\n",
    "\n",
    "        predictions = deepseek.batch_pcl_classify(texts, min(BATCH_SIZE, len(dev_df) - index))\n",
    "        logging.info(f\"Predictions: {predictions}\")\n",
    "        # Append a new entry to new DataFrame\n",
    "        new_df = add_predictions_to_new_df(predictions, index, dev_df, new_df)\n",
    "        # Save the new dataframe\n",
    "        new_df.to_csv(SAVE_CSV_FILEPATH, index=False, encoding=\"utf-8\")\n",
    "        # Increment index by batch size\n",
    "        index += BATCH_SIZE\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        index += BATCH_SIZE\n",
    "\n",
    "# Read the new csv file that contains the model's predictions\n",
    "preds_df = pd.read_csv(SAVE_CSV_FILEPATH)\n",
    "preds_df = pd.DataFrame(preds_df).reset_index(drop=True)\n",
    "\n",
    "# Get the predictions\n",
    "model_preds = preds_df[\"prediction\"]\n",
    "# Get the original labels\n",
    "labels  = preds_df[\"label\"]\n",
    "# Calculate F1 score\n",
    "f1_score = calc_f1(model_preds, labels)\n",
    "print(f\"The F1 score for DeepSeek v3 PCL binary classifaction: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrot import Parrot\n",
    "\n",
    "\n",
    "class ParrotParaphraser:\n",
    "    def __init__(self):\n",
    "        self.model = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\")\n",
    "\n",
    "    # Function to paraphrase a sentence using Parrot pretrained model\n",
    "    def paraphrase(self, sentence):\n",
    "        return self.model.augment(input_phrase=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_PARAPHRASES = 6\n",
    "\n",
    "def add_text_entry_to_new_df(sentence, index, old_df, new_df):\n",
    "    print(\"Adding text of index\", index)\n",
    "    entry = old_df.iloc[[index]].copy()\n",
    "    print(\"Entry:\", entry)\n",
    "    entry.at[entry.index[0], 'text'] = sentence\n",
    "    return pd.concat([new_df, entry], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a new empty dataframe\n",
    "new_df = pd.DataFrame(columns=train_df.columns)\n",
    "\n",
    "# Initialise Parrot paraphrasing model\n",
    "parrot_paraphraser = ParrotParaphraser()\n",
    "\n",
    "# Filter and get only the entries that are labeled as PCL\n",
    "pcl_df = train_df[train_df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "for index, _ in pcl_df.iterrows():\n",
    "    original_text = pcl_df.at[index, \"text\"]\n",
    "    paraphrases = [original_text]\n",
    "    output = parrot_paraphraser.paraphrase(original_text)\n",
    "    # Add the original entry to the new dataframe\n",
    "    new_df = add_text_entry_to_new_df(original_text, index, pcl_df, new_df)\n",
    "    # If sentence cannot be paraphrased, skip\n",
    "    if not output: \n",
    "        continue\n",
    "    for paraphrase in output:\n",
    "        if len(paraphrases) >= MAX_NUM_PARAPHRASES:\n",
    "            break\n",
    "        if paraphrase not in paraphrases:\n",
    "            paraphrases.append(paraphrase)\n",
    "            # Append a new entry to new DataFrame\n",
    "            new_df = add_text_entry_to_new_df(paraphrase[0], index, pcl_df, new_df)\n",
    "\n",
    "# Add all original non pcl entries to the new dataframe\n",
    "non_pcl_df = train_df[train_df['label'] == 0]\n",
    "new_df = pd.concat([new_df, non_pcl_df], ignore_index=True)\n",
    "\n",
    "# Save the new dataframe\n",
    "new_df.to_csv(\"data/train_set_cleaned_paraphrase_upsampled.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, fn):\n",
    "    if (tp + 0.5 * (fp + fn)) == 0:\n",
    "        return 0\n",
    "    return tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "def augment_text(row, deletion_prob=0.05, swap_prob=0.3):\n",
    "    # Tokenize the text\n",
    "    words = row['text'].split()\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if random.random() < swap_prob:\n",
    "            swap_i = random.randint(0, len(words) - 1)\n",
    "            if swap_i != i:  # Ensure not swapping with itself\n",
    "                words[i], words[swap_i] = words[swap_i], words[i]\n",
    "    \n",
    "    # Apply deletion\n",
    "    words = [word for word in words if random.random() >= deletion_prob]\n",
    "    \n",
    "    # Reconstruct the augmented text\n",
    "    augmented_text = ' '.join(words)\n",
    "    return augmented_text\n",
    "\n",
    "def train_deberta(learning_rate, batch_size, num_epochs, weight_decay, dropout, num_layers_unfrozen, augment_warmup_epochs,\n",
    "                  train_file='data/train_paraphrase_upsampled.csv', test_file='data/dev_set.csv',\n",
    "                  save_path=None, test_results_path='dev.txt'):\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    # Model configuration with hyperparameters\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_best_model\": False,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_model_every_epoch\": False,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "        \"use_multiprocessing_for_evaluation\": False,\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"fp16\": False,  # Ensure FP16 is disabled\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "\n",
    "    # Initialize DeBERTa model\n",
    "    model = ClassificationModel(\n",
    "        \"deberta\",\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=2,\n",
    "        args=model_args,\n",
    "    )\n",
    "\n",
    "    # Unfreeze the last `num_layers_unfrozen` layers + classifier head\n",
    "    model_layers = list(model.model.deberta.encoder.layer)\n",
    "    num_total_layers = len(model_layers)\n",
    "    layers_to_unfreeze = min(num_layers_unfrozen, num_total_layers)\n",
    "\n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = False  # Freeze everything first\n",
    "\n",
    "    for i in range(num_total_layers - layers_to_unfreeze, num_total_layers):\n",
    "        for param in model_layers[i].parameters():\n",
    "            param.requires_grad = True  # Unfreeze selected layers\n",
    "\n",
    "    # Ensure classifier head is always trainable\n",
    "    for name, param in model.model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    cols = [\"text\", \"label\"]\n",
    "    \n",
    "    # Train for the required number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        _train_df = train_df.copy()\n",
    "\n",
    "        if epoch >= augment_warmup_epochs:\n",
    "            _train_df[\"text\"] = _train_df.apply(lambda row: augment_text(row), axis=1)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train_model(_train_df[cols])\n",
    "        \n",
    "    print(\"Evaluating on dev set\")\n",
    "    preds, _ = model.predict(test_df[cols])\n",
    "\n",
    "    with open(test_results_path, 'w+') as f:\n",
    "        for pred in preds:\n",
    "            f.write(pred + '\\n')\n",
    "    \n",
    "    if save_path:\n",
    "        print(\"Saving final model to\", save_path)\n",
    "        model.model.save_pretrained(save_path)\n",
    "        model.tokenizer.save_pretrained(save_path)\n",
    "        model.config.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best hyperparameters\n",
    "train_deberta(\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=32,\n",
    "    num_epochs=12,\n",
    "    weight_decay=0.0,\n",
    "    dropout=0.0,\n",
    "    num_layers_unfrozen=3,\n",
    "    augment_warmup_epochs=5,\n",
    "    save_path='final_model',\n",
    "    test_results_path='dev.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
